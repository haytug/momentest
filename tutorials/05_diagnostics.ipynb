{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial 5: Diagnostics and Visualization\n",
                "\n",
                "This tutorial covers **diagnostic tools** for GMM/SMM estimation - how to visualize results, check model fit, and diagnose potential issues.\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "1. Visualizing the objective function landscape\n",
                "2. Checking moment fit\n",
                "3. Diagnosing identification issues\n",
                "4. Analyzing optimization convergence\n",
                "5. Using the J-test for model specification\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- Completed Tutorials 1-4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from momentest import (\n",
                "    smm_estimate,\n",
                "    gmm_estimate,\n",
                "    SMMEngine,\n",
                "    GMMEngine,\n",
                "    EstimationSetup,\n",
                "    estimate,\n",
                "    j_test,\n",
                "    linear_iv,\n",
                "    load_econ381,\n",
                "    # Visualization functions\n",
                "    plot_objective_landscape,\n",
                "    plot_moment_contributions,\n",
                "    plot_identification,\n",
                "    plot_marginal_objective,\n",
                "    plot_moment_comparison,\n",
                "    plot_convergence,\n",
                "    summary,\n",
                ")\n",
                "\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup: Truncated Normal Example\n",
                "\n",
                "We'll use the truncated normal example for most visualizations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import scipy.stats as sts\n",
                "\n",
                "# Load data\n",
                "dataset = load_econ381()\n",
                "data = dataset['data']\n",
                "cut_lb, cut_ub = dataset['bounds']\n",
                "N = len(data)\n",
                "\n",
                "# Target moments\n",
                "data_mean = data.mean()\n",
                "data_var = data.var()\n",
                "data_moments = np.array([data_mean, data_var])\n",
                "\n",
                "# Simulation functions\n",
                "def trunc_norm_draws(unif_vals, mu, sigma, cut_lb, cut_ub):\n",
                "    sigma = max(sigma, 1e-6)\n",
                "    cut_lb_cdf = sts.norm.cdf(cut_lb, loc=mu, scale=sigma)\n",
                "    cut_ub_cdf = sts.norm.cdf(cut_ub, loc=mu, scale=sigma)\n",
                "    cdf_range = cut_ub_cdf - cut_lb_cdf\n",
                "    if cdf_range < 1e-10:\n",
                "        return np.full_like(unif_vals, (cut_lb + cut_ub) / 2)\n",
                "    unif_scaled = unif_vals * cdf_range + cut_lb_cdf\n",
                "    unif_scaled = np.clip(unif_scaled, cut_lb_cdf + 1e-10, cut_ub_cdf - 1e-10)\n",
                "    return np.clip(sts.norm.ppf(unif_scaled, loc=mu, scale=sigma), cut_lb, cut_ub)\n",
                "\n",
                "def sim_func(theta, shocks):\n",
                "    mu, sigma = theta\n",
                "    sigma = max(sigma, 1.0)\n",
                "    return trunc_norm_draws(shocks, mu, sigma, cut_lb, cut_ub)\n",
                "\n",
                "def moment_func(sim_data):\n",
                "    return np.column_stack([np.mean(sim_data, axis=1), np.var(sim_data, axis=1)])\n",
                "\n",
                "# Create engine\n",
                "engine = SMMEngine(\n",
                "    k=2, p=2, n_sim=300, shock_dim=N,\n",
                "    sim_func=sim_func, moment_func=moment_func, seed=42\n",
                ")\n",
                "\n",
                "# Estimate\n",
                "result = smm_estimate(\n",
                "    sim_func=sim_func, moment_func=moment_func,\n",
                "    data_moments=data_moments.tolist(),\n",
                "    bounds=[(0, 1000), (1, 500)],\n",
                "    n_sim=300, shock_dim=N, seed=42, weighting=\"optimal\",\n",
                ")\n",
                "\n",
                "print(f\"Estimates: μ={result.theta[0]:.2f}, σ={result.theta[1]:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Objective Function Landscape\n",
                "\n",
                "Visualizing the objective function helps understand:\n",
                "- Is there a unique minimum?\n",
                "- Are there local minima?\n",
                "- How \"flat\" is the objective near the minimum?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get weighting matrix\n",
                "_, S = engine.moments(result.theta)\n",
                "W = np.linalg.inv(S)\n",
                "\n",
                "# Plot objective landscape\n",
                "fig = plot_objective_landscape(\n",
                "    engine=engine,\n",
                "    theta_hat=result.theta,\n",
                "    data_moments=data_moments,\n",
                "    W=W,\n",
                "    param_indices=(0, 1),\n",
                "    param_names=[\"μ\", \"σ\"],\n",
                "    n_points=40,\n",
                "    scale=0.3,\n",
                "    plot_type=\"both\",\n",
                ")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Interpreting the Landscape\n",
                "\n",
                "- **Well-identified**: Clear, unique minimum (bowl shape)\n",
                "- **Poorly identified**: Flat regions, multiple minima, or ridges\n",
                "- **Red star**: The estimated parameters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Marginal Objective Functions\n",
                "\n",
                "Look at how the objective changes when varying one parameter at a time:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot marginal objectives\n",
                "fig = plot_marginal_objective(\n",
                "    engine=engine,\n",
                "    theta_hat=result.theta,\n",
                "    data_moments=data_moments,\n",
                "    W=W,\n",
                "    param_names=[\"μ\", \"σ\"],\n",
                "    n_points=50,\n",
                "    scale=0.3,\n",
                ")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Moment Contributions\n",
                "\n",
                "See how each moment contributes to the objective:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot moment contributions\n",
                "fig = plot_moment_contributions(\n",
                "    engine=engine,\n",
                "    theta_hat=result.theta,\n",
                "    data_moments=data_moments,\n",
                "    W=W,\n",
                "    param_index=0,  # Vary μ\n",
                "    param_names=[\"μ\", \"σ\"],\n",
                "    moment_names=[\"Mean\", \"Variance\"],\n",
                "    n_points=50,\n",
                "    scale=0.3,\n",
                ")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Interpreting Moment Contributions\n",
                "\n",
                "- **Informative moments**: Large contribution, steep slope\n",
                "- **Uninformative moments**: Small contribution, flat\n",
                "- **At estimate**: All contributions should be near zero"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Identification Analysis\n",
                "\n",
                "The Jacobian matrix $D = \\partial m / \\partial \\theta$ shows which moments identify which parameters:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot identification (Jacobian heatmap)\n",
                "fig = plot_identification(\n",
                "    engine=engine,\n",
                "    theta_hat=result.theta,\n",
                "    param_names=[\"μ\", \"σ\"],\n",
                "    moment_names=[\"Mean\", \"Variance\"],\n",
                ")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Interpreting the Jacobian\n",
                "\n",
                "- **Left panel**: Raw Jacobian values\n",
                "- **Right panel**: Normalized sensitivity (which moments respond most to each parameter)\n",
                "- **Good identification**: Each parameter affects at least one moment strongly\n",
                "- **Poor identification**: A column is all zeros (parameter doesn't affect any moment)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Moment Fit Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot moment comparison\n",
                "fig = plot_moment_comparison(\n",
                "    data_moments=data_moments,\n",
                "    model_moments=result.sim_moments,\n",
                "    moment_names=[\"Mean\", \"Variance\"],\n",
                ")\n",
                "plt.suptitle(\"Data vs Model Moments\", y=1.02)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Full Summary Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print comprehensive summary\n",
                "print(summary(\n",
                "    theta=result.theta,\n",
                "    se=result.se,\n",
                "    objective=result.objective,\n",
                "    data_moments=data_moments,\n",
                "    model_moments=result.sim_moments,\n",
                "    k=2,\n",
                "    p=2,\n",
                "    n=300,\n",
                "    converged=result.converged,\n",
                "    param_names=[\"μ\", \"σ\"],\n",
                "    moment_names=[\"Mean\", \"Variance\"],\n",
                "    method=\"SMM\",\n",
                "))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. J-Test for Overidentification\n",
                "\n",
                "Let's use a GMM example with overidentification to demonstrate the J-test:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate IV data\n",
                "dgp = linear_iv(n=500, seed=42, beta0=1.0, beta1=2.0, rho=0.5)\n",
                "dgp.data['Z2'] = dgp.data['Z']**2\n",
                "dgp.data['Z3'] = dgp.data['Z']**3\n",
                "\n",
                "def moment_func_gmm(data, theta):\n",
                "    beta0, beta1 = theta\n",
                "    residual = data['Y'] - beta0 - beta1 * data['X']\n",
                "    return np.column_stack([\n",
                "        residual,\n",
                "        residual * data['Z'],\n",
                "        residual * data['Z2'],\n",
                "        residual * data['Z3'],\n",
                "    ])\n",
                "\n",
                "# Estimate\n",
                "result_gmm = gmm_estimate(\n",
                "    data=dgp.data,\n",
                "    moment_func=moment_func_gmm,\n",
                "    bounds=[(-10, 10), (-10, 10)],\n",
                "    k=4,\n",
                "    weighting=\"optimal\",\n",
                ")\n",
                "\n",
                "print(f\"GMM estimates: β₀={result_gmm.theta[0]:.4f}, β₁={result_gmm.theta[1]:.4f}\")\n",
                "print(f\"True values:   β₀={dgp.true_theta[0]:.4f}, β₁={dgp.true_theta[1]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# J-test\n",
                "j_result = j_test(\n",
                "    objective=result_gmm.objective,\n",
                "    n=dgp.n,\n",
                "    k=4,\n",
                "    p=2,\n",
                ")\n",
                "\n",
                "print(j_result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Interpreting the J-Test\n",
                "\n",
                "- **H₀**: All moment conditions are valid\n",
                "- **Fail to reject** (high p-value): Model is correctly specified\n",
                "- **Reject** (low p-value): At least one moment condition is invalid\n",
                "\n",
                "Common reasons for rejection:\n",
                "1. Invalid instruments (exclusion restriction violated)\n",
                "2. Model misspecification\n",
                "3. Heteroskedasticity not accounted for"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Convergence Diagnostics\n",
                "\n",
                "Let's look at how the optimization progressed:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run estimation with history tracking\n",
                "setup = EstimationSetup(\n",
                "    mode=\"SMM\", model_name=\"truncated_normal\", moment_type=\"mean_var\",\n",
                "    k=2, p=2, n_sim=300, shock_dim=N, seed=42, weighting=\"optimal\"\n",
                ")\n",
                "\n",
                "result_with_history = estimate(\n",
                "    setup=setup,\n",
                "    data_moments=data_moments,\n",
                "    bounds=[(0, 1000), (1, 500)],\n",
                "    n_global=50,\n",
                "    engine=engine,\n",
                ")\n",
                "\n",
                "print(f\"Total evaluations: {result_with_history.n_evals}\")\n",
                "print(f\"History length: {len(result_with_history.history)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot convergence\n",
                "fig = plot_convergence(\n",
                "    engine=engine,\n",
                "    history=result_with_history.history,\n",
                "    data_moments=data_moments,\n",
                "    W=result_with_history.W,\n",
                "    param_indices=(0, 1),\n",
                "    param_names=[\"μ\", \"σ\"],\n",
                "    n_points=40,\n",
                "    scale=0.3,\n",
                ")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Diagnostic Checklist\n",
                "\n",
                "When estimating a model, check:\n",
                "\n",
                "### Before Estimation\n",
                "- [ ] Moments are well-defined and finite\n",
                "- [ ] Parameter bounds are reasonable\n",
                "- [ ] Enough simulations (n_sim ≥ 100)\n",
                "\n",
                "### After Estimation\n",
                "- [ ] Optimization converged (`result.converged == True`)\n",
                "- [ ] Objective is small (moments are matched)\n",
                "- [ ] Standard errors are finite and reasonable\n",
                "- [ ] J-test doesn't reject (if overidentified)\n",
                "\n",
                "### Visualization Checks\n",
                "- [ ] Objective landscape has clear minimum\n",
                "- [ ] No flat regions (identification)\n",
                "- [ ] Moments are well-matched\n",
                "- [ ] Convergence path is smooth"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Exercises\n",
                "\n",
                "### Exercise 1: Weak Identification\n",
                "Create a model where one parameter is weakly identified. What do the diagnostics show?\n",
                "\n",
                "### Exercise 2: Model Misspecification\n",
                "Fit a truncated normal to data from a different distribution. Does the J-test detect it?\n",
                "\n",
                "### Exercise 3: Multiple Local Minima\n",
                "Create a model with multiple local minima. How does the objective landscape look?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2 starter: Generate data from a different distribution\n",
                "# and try to fit truncated normal\n",
                "\n",
                "# Generate from beta distribution (not normal!)\n",
                "np.random.seed(42)\n",
                "data_beta = 450 * np.random.beta(2, 5, size=161)  # Skewed!\n",
                "\n",
                "# Compute moments\n",
                "data_moments_beta = np.array([data_beta.mean(), data_beta.var()])\n",
                "\n",
                "# Fit truncated normal\n",
                "result_misspec = smm_estimate(\n",
                "    sim_func=sim_func, moment_func=moment_func,\n",
                "    data_moments=data_moments_beta.tolist(),\n",
                "    bounds=[(0, 1000), (1, 500)],\n",
                "    n_sim=300, shock_dim=161, seed=42, weighting=\"optimal\",\n",
                ")\n",
                "\n",
                "print(f\"Fitted to beta data: μ={result_misspec.theta[0]:.2f}, σ={result_misspec.theta[1]:.2f}\")\n",
                "print(f\"Objective: {result_misspec.objective:.4f}\")\n",
                "print(\"\\nThe model fits the mean and variance, but the shape is wrong!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this tutorial, you learned:\n",
                "\n",
                "1. **Objective landscape**: Visualize the optimization surface\n",
                "2. **Moment contributions**: See which moments drive the fit\n",
                "3. **Identification**: Check the Jacobian for weak identification\n",
                "4. **Moment fit**: Compare data vs model moments\n",
                "5. **J-test**: Test overidentifying restrictions\n",
                "6. **Convergence**: Track optimization progress\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "- Always visualize the objective landscape\n",
                "- Check identification before trusting estimates\n",
                "- Use J-test for overidentified models\n",
                "- Diagnostics help catch problems early\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- **Tutorial 6**: Advanced structural models (DDC, dynamic models)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}